# daily_elt_job.py
# ----------------------------
# (Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ© - DWH)
# Ù…Ù‡Ù…ØªÙ‡: Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù… (JSON) Ù…Ù† Data LakeØŒ ÙˆØªØ­ÙˆÙŠÙ„Ù‡Ø§ØŒ
# ÙˆØ­ÙØ¸Ù‡Ø§ Ù…Ø¨Ø§Ø´Ø±Ø© ÙÙŠ Ø¬Ø¯Ø§ÙˆÙ„ Synapse Dedicated SQL Pool (DWH).
# ----------------------------

import sys
from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, to_timestamp, lit, explode, split, 
    min, max, sum, count, first, when
)
from pyspark.sql.types import (
    StructType, StructField, StringType, IntegerType, 
    DoubleType, BooleanType, TimestampType
)

# ----------------------------
# âš ï¸ 1. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§ØªØµØ§Ù„ 
# ----------------------------

# 1. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ Synapse DWH (Ø§Ù„Ù€ Dedicated SQL Pool)
# (Ø§Ø³ØªØ¨Ø¯Ù„ Ø§Ù„Ù‚ÙŠÙ…!)
SYNAPSE_SERVER = "radar-synapse-workspace.sql.azuresynapse.net"  # â¬…ï¸ (Ø§Ø³Ù… Ø§Ù„Ø³ÙŠØ±ÙØ±)
SYNAPSE_DB = "RadarDW"  # â¬…ï¸ (Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ø°ÙŠ Ø§Ø®ØªØ±ØªÙ‡ Ù„Ù„Ù€ SQL Pool)
SYNAPSE_USER = "sqladminuser"  # â¬…ï¸(Ø§Ù„ÙŠÙˆØ²Ø± Ø§Ù„Ø®Ø§Øµ Ø¨Ù€ SQL)
SYNAPSE_PASS = "YourPassword!"  # â¬…ï¸ (ÙƒÙ„Ù…Ø© Ø§Ù„Ù…Ø±ÙˆØ±)

# Ù‡Ø°Ø§ Ø§Ù„Ø³Ø·Ø± ÙŠØ¨Ù†ÙŠ "Ø¹Ù†ÙˆØ§Ù†" Ø§Ù„Ø§ØªØµØ§Ù„
SYNAPSE_SQL_URL = f"jdbc:sqlserver://{SYNAPSE_SERVER}:1433;database={SYNAPSE_DB};user={SYNAPSE_USER};password={SYNAPSE_PASS}"
SYNAPSE_TABLE_OPTIONS = {
    "driver": "com.microsoft.sqlserver.jdbc.SQLServerDriver"
}

# 2. Ù…Ø³Ø§Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù… (JSON)
# (Ù‡Ø°Ø§ Ø§Ù„Ù…Ø³Ø§Ø± ÙŠÙ‚Ø±Ø£ Ù…Ù„ÙØ§Øª .json ÙÙ‚Ø·ØŒ ÙƒÙ…Ø§ Ø§ØªÙÙ‚Ù†Ø§)
DATA_LAKE_RAW_PATH = "abfss://radarcont@radardatalake1.dfs.core.windows.net/*.json"

# ----------------------------
# Ø¨ÙŠØ§Ù†Ø§Øª Ø«Ø§Ø¨ØªØ© (Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù…Ø®Ø§Ù„ÙØ§Øª)
# ----------------------------
VIOLATION_TYPES_DATA = [
    ("SPD_001", "Moderate Speeding", 500),
    ("SPD_002", "Severe Speeding", 1000),
    ("SBL_001", "No Seat Belt", 300),
    ("PHN_001", "Phone Usage", 400),
]

# ----------------------------
# Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø©
# ----------------------------

def get_spark_session():
    """Ø¥Ù†Ø´Ø§Ø¡ Ø£Ùˆ Ø¬Ù„Ø¨ Spark Session."""
    return SparkSession.builder \
        .appName("Radar Batch ELT Job (DWH)") \
        .getOrCreate()

def save_to_synapse(df, table_name, mode="append"):
    """
    Ø¯Ø§Ù„Ø© Ù„Ø­ÙØ¸ DataFrame ÙÙŠ Synapse DWH Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… JDBC.
    """
    print(f"ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ÙƒØªØ§Ø¨Ø© ÙÙŠ Ø¬Ø¯ÙˆÙ„ DWH: {table_name}, Ø§Ù„ÙˆØ¶Ø¹: {mode}")
    try:
        df.write \
          .format("jdbc") \
          .options(**SYNAPSE_TABLE_OPTIONS) \
          .option("url", SYNAPSE_SQL_URL) \
          .option("dbtable", table_name) \
          .mode(mode) \
          .save()
        print(f"âœ… ØªÙ…Øª Ø§Ù„ÙƒØªØ§Ø¨Ø© Ø¨Ù†Ø¬Ø§Ø­ ÙÙŠ: {table_name}")
    except Exception as e:
        print(f"âŒ ÙØ´Ù„ ÙÙŠ Ø§Ù„ÙƒØªØ§Ø¨Ø© Ù„Ù€ {table_name}: {e}")

# ----------------------------
# Ù…Ù†Ø·Ù‚ Ø§Ù„Ù€ ELT (Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©)
# ----------------------------

def load_raw_data(spark, input_path):
    """
    1. Ù‚Ø±Ø§Ø¡Ø© Ø¨ÙŠØ§Ù†Ø§Øª JSON Ø§Ù„Ø®Ø§Ù… Ù…Ù† Data Lake.
    """
    print(f"ğŸ“¥ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù… Ù…Ù†: {input_path}")
    
    raw_schema = StructType([
        StructField("id", StringType(), True),
        StructField("timestamp", StringType(), True),
        StructField("journey_id", StringType(), True),
        StructField("plate", StringType(), True),
        StructField("color", StringType(), True),
        StructField("driver_profile", StringType(), True),
        StructField("route_id", StringType(), True),
        StructField("radar_id", StringType(), True),
        StructField("radar_index", IntegerType(), True),
        StructField("lat", DoubleType(), True),
        StructField("lon", DoubleType(), True),
        StructField("speed", IntegerType(), True),
        StructField("speed_limit", IntegerType(), True),
        StructField("seat_belt", BooleanType(), True),
        StructField("phone_usage", BooleanType(), True),
        StructField("is_violation", BooleanType(), True),
        StructField("violation_codes", StringType(), True),
        StructField("total_fine", IntegerType(), True),
        StructField("segment_distance_km", DoubleType(), True)
    ])

    df = spark.read.format("json") \
             .schema(raw_schema) \
             .load(input_path)
    
    df = df.withColumn("timestamp_dt", to_timestamp(col("timestamp"), "yyyy-MM-dd'T'HH:mm:ss.SSSSSS'Z'")) \
           .dropDuplicates(["id"])
           
    df.cache() 
    count = df.count()
    print(f"ğŸ“Š ØªÙ… Ù‚Ø±Ø§Ø¡Ø© {count} Ø³Ø¬Ù„ Ø®Ø§Ù….")
    return df

def process_logs_and_violations(spark, df_raw):
    """
    2. Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØ­ÙØ¸ Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„Ø³Ø¬Ù„Ø§Øª (Logs & Violations).
    (Ù†Ø³ØªØ®Ø¯Ù… "append" Ù‡Ù†Ø§ Ù„Ø£Ù†Ù†Ø§ Ù†Ø±ÙŠØ¯ ÙÙ‚Ø· Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© ÙƒÙ„ ÙŠÙˆÙ…)
    """
    print(" processing logs and violations (Mode: append)...")

    # --- radar_logs ---
    df_radar_logs = df_raw.select(
        col("id"), 
        col("journey_id"), "plate", "speed", "speed_limit", "color", "radar_id",
        "radar_index", "lat", "lon", "seat_belt", "phone_usage", "is_violation",
        "violation_codes", "total_fine", "segment_distance_km",
        col("timestamp_dt").alias("timestamp")
    )
  
    save_to_synapse(df_radar_logs, "radar_logs", mode="append")

    # --- violations ---
    violation_schema = StructType([
        StructField("reason_code", StringType()),
        StructField("reason_name", StringType()),
        StructField("fine_amount", IntegerType())
    ])
    df_violation_lookup = spark.createDataFrame(VIOLATION_TYPES_DATA, violation_schema)

    df_violations_raw = df_raw.filter(col("is_violation") == True) \
                              .filter(col("violation_codes").isNotNull())
    
    df_violations_exploded = df_violations_raw.select(
        col("journey_id"),
        col("plate"),
        col("timestamp_dt").alias("timestamp"),
        explode(split(col("violation_codes"), ";")).alias("reason_code")
    )
    
    df_violations_final = df_violations_exploded.join(
        df_violation_lookup,
        df_violations_exploded.reason_code == df_violation_lookup.reason_code,
        "left"
    ).select(
        col("journey_id"),
        col("plate"),
        col("timestamp"),
        col("reason_name").alias("reason"), 
        col("fine_amount").alias("fine")      
    )
    
    save_to_synapse(df_violations_final, "violations", mode="append")

def process_dims_and_facts(df_raw):
    """
    3. Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØ­ÙØ¸ Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ ÙˆØ§Ù„Ø­Ù‚Ø§Ø¦Ù‚ (Dims & Facts).
    (Ù†Ø³ØªØ®Ø¯Ù… "overwrite" Ù‡Ù†Ø§ Ù„Ù„ØªØ¨Ø³ÙŠØ·)
    """
    print(" processing dims and facts (Mode: overwrite)...")

    # --- dim_vehicles ---
    df_vehicles = df_raw.select("plate", "color", "timestamp_dt") \
                        .groupBy("plate") \
                        .agg(
                            first("color").alias("color"),
                            min("timestamp_dt").alias("created_at")
                        )
    
    save_to_synapse(df_vehicles, "dim_vehicles", mode="overwrite")

    # --- fact_journeys ---
    df_facts = df_raw.groupBy("journey_id") \
                     .agg(
                         first("plate").alias("plate"),
                         first("route_id").alias("route_id"),
                         first("driver_profile").alias("driver_profile"),
                         min("timestamp_dt").alias("start_time"),
                         max("timestamp_dt").alias("end_time"),
                         sum("segment_distance_km").alias("total_distance"),
                         sum(when(col("is_violation") == True, 1).otherwise(0)).alias("total_violations"),
                         sum("total_fine").alias("total_fines")
                     )
    
    save_to_synapse(df_facts, "fact_journeys", mode="overwrite")

# ----------------------------
# Main execution
# ----------------------------
def main():
    try:
        spark
    except NameError:
        spark = get_spark_session()
    
    df_raw = load_raw_data(spark, DATA_LAKE_RAW_PATH)
    
    process_logs_and_violations(spark, df_raw)
    
    process_dims_and_facts(df_raw)
    
    print("âœ… ELT Batch Job (DWH) Completed Successfully.")

if __name__ == "__main__":
    main()