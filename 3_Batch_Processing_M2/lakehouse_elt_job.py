# ( Optional code )
# Ø§Ù„ÙƒÙˆØ¯ Ø¯Ø§ ÙƒØ§Ù† Ù…Ø¤Ù‚Øª Ø¨ÙŠØ­ÙØ¸ Ø§Ù„Ø¯Ø§ØªØ§ Ø§Ù„ÙŠ Ø§ØªØ¹Ù…Ù„Ù‡Ø§ ØªØ±Ø§Ù†ÙÙˆØ±Ù…ÙŠØ´Ù†Ø² Ù datalakehouse
# ÙˆÙ„Ø§ÙƒÙ† Ø§Ù„Ø¨Ø±ÙˆØ¬ÙƒØª Ù…ØµÙ…Ù… Ø§Ù†Ùˆ ÙŠØ®Ø²Ù† Ø§Ù„Ø¯Ø§ØªØ§ Ù data warehouse
# Ø¹Ø´Ø§Ù† ÙƒØ¯Ø§ Ø§Ù„ÙƒÙˆØ¯ Ø¯Ø§ Ù…Ø´ Ù‡ÙŠØ³ØªØ®Ø¯Ù… Ù Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ù…Ù† Ø§Ù„Ø¨Ø±ÙˆØ¬
# --------------------------------------------------------------

# daily_elt_job_LAKEHOUSE.py
# ----------------------------
# (Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© - Ù…Ø¹ Ø¥ØµÙ„Ø§Ø­ Ù…Ø³Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©)
# Ù…Ù‡Ù…ØªÙ‡: Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù… (JSON) Ù…Ù† Data LakeØŒ
# ÙˆØªØ­ÙˆÙŠÙ„Ù‡Ø§ØŒ ÙˆØ­ÙØ¸Ù‡Ø§ ÙƒÙ…Ù„ÙØ§Øª Parquet Ù†Ø¸ÙŠÙØ© ÙÙŠ Ù…Ø¬Ù„Ø¯ Ø¢Ø®Ø± Ø¨Ø§Ù„Ù€ Data Lake.
# ----------------------------

import sys
from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, to_timestamp, lit, explode, split, 
    min, max, sum, count, first, when
)
from pyspark.sql.types import (
    StructType, StructField, StringType, IntegerType, 
    DoubleType, BooleanType, TimestampType
)

# ----------------------------
# âš ï¸ 1. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø³Ø§Ø± 
# ----------------------------

# 1. Ù…Ø³Ø§Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù… (JSON)
# (Ù‡Ø°Ø§ Ù‡Ùˆ Ø§Ù„Ù…Ø³Ø§Ø± Ø­ÙŠØ« ÙŠØ­ÙØ¸ Stream Analytics Ø§Ù„Ù…Ù„ÙØ§Øª)
DATA_LAKE_RAW_PATH = "abfss://radarcont@radardatalake1.dfs.core.windows.net/*.json"

# 2. Ù…Ø³Ø§Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Ø¸ÙŠÙØ© (Parquet)
# (Ù‡Ø°Ø§ Ù‡Ùˆ "Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹" Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø¬Ø§Ù†ÙŠ. Ø³Ù†Ù†Ø´Ø¦ Ù…Ø¬Ù„Ø¯Ù‹Ø§ Ø¬Ø¯ÙŠØ¯Ù‹Ø§ Ø§Ø³Ù…Ù‡ clean-tables)
DATA_LAKE_CLEAN_PATH = "abfss://radarcont@radardatalake1.dfs.core.windows.net/clean-tables/"


# ----------------------------
# Ø¨ÙŠØ§Ù†Ø§Øª Ø«Ø§Ø¨ØªØ© (Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù…Ø®Ø§Ù„ÙØ§Øª)
# ----------------------------
VIOLATION_TYPES_DATA = [
    ("SPD_001", "Moderate Speeding", 500),
    ("SPD_002", "Severe Speeding", 1000),
    ("SBL_001", "No Seat Belt", 300),
    ("PHN_001", "Phone Usage", 400),
]

# ----------------------------
# Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø©
# ----------------------------

def get_spark_session():
    """Ø¥Ù†Ø´Ø§Ø¡ Ø£Ùˆ Ø¬Ù„Ø¨ Spark Session."""
    return SparkSession.builder \
        .appName("Radar Batch ELT Job (Lakehouse)") \
        .getOrCreate()


# ----------------------------
# Ù…Ù†Ø·Ù‚ Ø§Ù„Ù€ ELT (Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©)
# ----------------------------

def load_raw_data(spark, input_path):
    """
    1. Ù‚Ø±Ø§Ø¡Ø© Ø¨ÙŠØ§Ù†Ø§Øª JSON Ø§Ù„Ø®Ø§Ù… Ù…Ù† Data Lake.
    """
    print(f"ğŸ“¥ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù… Ù…Ù†: {input_path}")
    
    # Ù‡Ø°Ø§ Ø§Ù„Ù…Ø®Ø·Ø· ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ·Ø§Ø¨Ù‚ ØªÙ…Ø§Ù…Ù‹Ø§ Ø§Ù„Ù€ JSON Ø§Ù„Ø°ÙŠ ÙŠØ±Ø³Ù„Ù‡ Ø§Ù„Ù…Ù†ØªØ¬
    raw_schema = StructType([
        StructField("id", StringType(), True),
        StructField("timestamp", StringType(), True),
        StructField("journey_id", StringType(), True),
        StructField("plate", StringType(), True),
        StructField("color", StringType(), True),
        StructField("driver_profile", StringType(), True),
        StructField("route_id", StringType(), True),
        StructField("radar_id", StringType(), True),
        StructField("radar_index", IntegerType(), True),
        StructField("lat", DoubleType(), True),
        StructField("lon", DoubleType(), True),
        StructField("speed", IntegerType(), True),
        StructField("speed_limit", IntegerType(), True),
        StructField("seat_belt", BooleanType(), True),
        StructField("phone_usage", BooleanType(), True),
        StructField("is_violation", BooleanType(), True),
        StructField("violation_codes", StringType(), True),
        StructField("total_fine", IntegerType(), True),
        StructField("segment_distance_km", DoubleType(), True)
    ])

    df = spark.read.format("json") \
             .schema(raw_schema) \
             .load(input_path)
    
    # ØªÙ†Ø¸ÙŠÙ Ø£Ø³Ø§Ø³ÙŠ
    df = df.withColumn("timestamp_dt", to_timestamp(col("timestamp"), "yyyy-MM-dd'T'HH:mm:ss.SSSSSS'Z'")) \
           .dropDuplicates(["id"])
           
    df.cache() 
    count = df.count()
    print(f"ğŸ“Š ØªÙ… Ù‚Ø±Ø§Ø¡Ø© {count} Ø³Ø¬Ù„ Ø®Ø§Ù….")
    return df

def process_logs_and_violations(spark, df_raw):
    """
    2. Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØ­ÙØ¸ Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„Ø³Ø¬Ù„Ø§Øª (Logs & Violations) ÙƒÙ…Ù„ÙØ§Øª Parquet.
    (Ù†Ø³ØªØ®Ø¯Ù… "overwrite" Ù„Ù„ØªØ¨Ø³ÙŠØ· ÙÙŠ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ)
    """
    print(" processing logs and violations (Mode: overwrite)...")

    # --- radar_logs ---
    df_radar_logs = df_raw.select(
        col("id"), 
        col("journey_id"), "plate", "speed", "speed_limit", "color", "radar_id",
        "radar_index", "lat", "lon", "seat_belt", "phone_usage", "is_violation",
        "violation_codes", "total_fine", "segment_distance_km",
        col("timestamp_dt").alias("timestamp")
    )
    df_radar_logs.write.mode("overwrite").parquet(DATA_LAKE_CLEAN_PATH + "radar_logs")
    print(f"âœ… ØªÙ… Ø­ÙØ¸ radar_logs ÙÙŠ: {DATA_LAKE_CLEAN_PATH}radar_logs")

    # --- violations ---
    violation_schema = StructType([
        StructField("reason_code", StringType()),
        StructField("reason_name", StringType()),
        StructField("fine_amount", IntegerType())
    ])
    df_violation_lookup = spark.createDataFrame(VIOLATION_TYPES_DATA, violation_schema)

    df_violations_raw = df_raw.filter(col("is_violation") == True) \
                              .filter(col("violation_codes").isNotNull())
    
    df_violations_exploded = df_violations_raw.select(
        col("journey_id"),
        col("plate"),
        col("timestamp_dt").alias("timestamp"),
        explode(split(col("violation_codes"), ";")).alias("reason_code")
    )
    
    df_violations_final = df_violations_exploded.join(
        df_violation_lookup,
        df_violations_exploded.reason_code == df_violation_lookup.reason_code,
        "left"
    ).select(
        col("journey_id"),
        col("plate"),
        col("timestamp"),
        col("reason_name").alias("reason"), 
        col("fine_amount").alias("fine")      
    )
    
    df_violations_final.write.mode("overwrite").parquet(DATA_LAKE_CLEAN_PATH + "violations")
    print(f"âœ… ØªÙ… Ø­ÙØ¸ violations ÙÙŠ: {DATA_LAKE_CLEAN_PATH}violations")

def process_dims_and_facts(df_raw):
    """
    3. Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØ­ÙØ¸ Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ ÙˆØ§Ù„Ø­Ù‚Ø§Ø¦Ù‚ (Dims & Facts) ÙƒÙ…Ù„ÙØ§Øª Parquet.
    (Ù†Ø³ØªØ®Ø¯Ù… "overwrite" Ù„Ù„ØªØ¨Ø³ÙŠØ·)
    """
    print(" processing dims and facts (Mode: overwrite)...")

    # --- dim_vehicles ---
    df_vehicles = df_raw.select("plate", "color", "timestamp_dt") \
                        .groupBy("plate") \
                        .agg(
                            first("color").alias("color"),
                            min("timestamp_dt").alias("created_at")
                        )
    
    df_vehicles.write.mode("overwrite").parquet(DATA_LAKE_CLEAN_PATH + "dim_vehicles")
    print(f"âœ… ØªÙ… Ø­ÙØ¸ dim_vehicles ÙÙŠ: {DATA_LAKE_CLEAN_PATH}dim_vehicles")

    # --- fact_journeys ---
    df_facts = df_raw.groupBy("journey_id") \
                     .agg(
                         first("plate").alias("plate"),
                         first("route_id").alias("route_id"),
                         first("driver_profile").alias("driver_profile"),
                         min("timestamp_dt").alias("start_time"),
                         max("timestamp_dt").alias("end_time"),
                         sum("segment_distance_km").alias("total_distance"),
                         sum(when(col("is_violation") == True, 1).otherwise(0)).alias("total_violations"),
                         sum("total_fine").alias("total_fines")
                     )
    
    df_facts.write.mode("overwrite").parquet(DATA_LAKE_CLEAN_PATH + "fact_journeys")
    print(f"âœ… ØªÙ… Ø­ÙØ¸ fact_journeys ÙÙŠ: {DATA_LAKE_CLEAN_PATH}fact_journeys")

# ----------------------------
# Main execution
# ----------------------------
def main():
   
    try:
        spark
    except NameError:
        spark = get_spark_session()
    
    df_raw = load_raw_data(spark, DATA_LAKE_RAW_PATH)
    
    process_logs_and_violations(spark, df_raw)
    
    process_dims_and_facts(df_raw)
    
    print("âœ… ELT Batch Job (Lakehouse) Completed Successfully.")
    
    
    spark.stop()

if __name__ == "__main__":
    main()